%==========================================
%
% Sibgrapi 2013 paper
% Leone, Barbagallo, Banquiero
%
%==========================================


% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference]{IEEEtran}

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.



\usepackage{epstopdf}


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\usepackage{subimages}
\setfigdir{figs}




% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/
\usepackage{amsthm}
\newtheorem{definition}{Definition}





% *** SPECIALIZED LIST PACKAGES ***
%
\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/



% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{hyperref}





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Vertex Discard Occlusion Culling}

%------------------------------------------------------------------------- 
% change the % on next lines to produce the final camera-ready version 
\newif\iffinal
%\finalfalse
\finaltrue
\newcommand{\jemsid}{99999}
%------------------------------------------------------------------------- 

% author names and affiliations
% use a multiple column layout for up to two different
% affiliations


\iffinal
  \author{%
    \IEEEauthorblockN{Matias N. Leone, Leandro R. Barbagallo, Mariano M. Banquiero}
    \IEEEauthorblockA{%
      Explotaci{\'o}n de GPUs y Gr{\'a}ficos Por Computadora - GIGC\\
      Departamento de Ingenier{\'i}a en Sistemas de Informaci{\'o}n, Universidad Tecnol{\'o}gica Nacional\\
      Buenos Aires, Argentina\\
      Email: \{mleone, lbarbagallo, mbanquiero\}@frba.utn.edu.ar}
  
  }
\else
  \author{Sibgrapi paper ID: \jemsid \\ }
\fi




%------------------------------------------------------------------------- 
% Special Sibgrapi teaser
\teaser{%
  \oneimage{%
	Left: The densely occluded scene as viewed from the camera. Middle: The Occlusion Culling algorithm avoids rendering completely occluded objects. Right: The simplified occluder set used for occlusion.}
	{.99}{teaser.png}
}
%------------------------------------------------------------------------- 



% make the title area
\maketitle


\begin{abstract}
Performing visibility determination in densely occluded environments is essential to avoid rendering unnecessary objects and achieve high frame rates. 
In this work we present an implementation of the image space Occlusion Culling algorithm done completely in GPU, avoiding the latency introduced by returning 
the visibility results to the CPU. 
Our algorithm utilizes the GPU rendering power to construct the Occlusion Map and then performs the image space visibility test by splitting the region of 
the screen space occludees into parallelizable blocks. 
Our implementation does not need special hardware extensions and the visibility results are accessible by GPU shaders. It can be applied with excellent 
results in scenes where pixel shaders alter the depth values of the pixels, without interfering with hardware Early Z culling methods. 
We demonstrate the benefits and show the results of this method in real-time densely occluded scenes.


\end{abstract}

\begin{IEEEkeywords}
Occlusion Culling; GPU; Visibility Determination

\end{IEEEkeywords}


\IEEEpeerreviewmaketitle


%==========================================
%==========================================


%==========================================
\section{Introduction}
%
Complex scenes with thousands of meshes and expensive shading computations are increasingly frequent in current real-time graphics applications. 
Although commodity hardware continues to increase its computational power every day, scenes like this cannot be directly supported at real-time frame rates. 
Optimization techniques are crucial in order to manage that kind of graphics complexity.\\

Frustum culling is a commonly used technique to avoid rendering meshes that are outside the viewing volume. 
These invisible models can be discarded at an early stage in the pipeline obviating expensive computation that will not contribute to the final image. 
Unfortunately it does not consider objects (occludees) that do not contribute to the final image because they are being blocked by others in front of them (occluders).\\

Because of this, several Occlusion Culling techniques were developed to overcome this limitation. 
Applications with expensive pixel shaders may greatly improve their performance by reducing fragments overdraw.\\

The Early Z Pass \cite{z_pre_pass} technique avoids computing unnecessary pixel shaders following a two step procedure. 
First it draws the entire scene in order to store in the Z-Buffer all the depth values of the scene visible points. 
Second the scene is drawn once more, but this time the GPU can early reject the occluded fragments based on already present depth values in the Z-Buffer. 
This way non visible pixel shaders are not executed.\\

This technique is used by many applications to reduce its pixel overdraw but its main limitation is that GPU cannot take advantage of the Early-Z \cite{early_z} or 
\cite{HyperZ} optimization when the pixel shader uses a depth writing operation \cite{z_correct_bump_1} \cite{z_correct_bump_2}. 
Since our method discards occluded objects before they get rasterized, no restrictions related to depth writing are imposed to pixel shaders.\\



%------------------------------------------------------------------------- 
\paragraph*{Contributions}
%
In this work we present a technique for solving Occlusion Culling in GPU, without the need for special hardware extensions or CPU read back. 
It includes a visibility test in the vertex shader of the application in order to discard those vertices that belong to occluded meshes. 
If the mesh is occluded then all its vertices can be discarded in the vertex shader, avoiding the rasterization step and the pixel computations. 
A previous step computes the visibility state of each mesh in the GPU and stores its result in an output texture called \emph{Occlusion Map}. 
This result is acquired after performing a highly parallelized overlap and depth test comparison.


%==========================================
\section{Related work}
%
There is a great amount of research conducted on Occlusion Culling. 
A classification and overview of all these methods is presented by Cohen-Or et al. \cite{survey_visibility}. 
Among those techniques the ones that work in point-space are Hierarchical Z-Buffer \cite{Hierarchical_zbuffer} and Hierarchical Occlusion Culling \cite{hom}.\\

On modern GPUs hardware occlusion queries \cite{occlusion_queries} provide a built-in way to determine if a draw call contributes to the current frame, 
but suffer from latency and stalling effects due to the CPU read back. 
To address this issue temporal coherence techniques are applied \cite{occlusion_queries_2}\cite{occlusion_queries_3}, but they require spatial hierarchies of objects to limit the number 
of issues queries.\\

Some newer hardware capabilities allow conditional rendering without CPU intervention like OpenGL conditional rendering  which is implemented as 
GL\_NV\_conditional\_render [CITAR] extension and DirectX 11 predicated rendering implemented as the ID3D11Predicate interface [CITAR]. 
These methods determine whether geometry should be processed or culled depending on the results of a previous draw call. 
Current hardware conditional rendering does not allow the GPU shaders to access the occlusion results, but Engelhard et al. \cite{visibility_queries} implement a method that do allow this. 
Other authors [CITAR 1] [CITAR 2] also implement HZB on GPU using available in the newer compute shaders.\\

More recently Nie{\ss}ner \cite{occlusion_culling_tessellation} proposes a patch primitive based approach to perform occlusion culling applying Hierarchical Z-Buffer and temporal coherence. 
In recent years, since CPUs increased the number of cores and the set of SIMD instructions were extended, some approaches performed point based Occlusion 
Culling such as HOM using highly optimized software rasterizers \cite{killzone_3} \cite{frostbite} [CITAR intel] \cite{cacic_occlusion_2}.


%==========================================
\section{Vertex Discard Occlusion Culling}
%

%------------------------------------------------------------------------- 
\subsection{Algorithm overview}
%
In our proposed method we perform a from point , image precision \cite{survey_visibility} occlusion culling process completely in GPU without the need for the CPU to read back the results. 
The method consists of a series of steps that must be followed by each frame to generate the \emph{Occlusion Map}, to perform the \emph{Visibility Test} to 
get the potential Visible Set and finally use those results already present in GPU in order to discard the all vertices of the occluded objects before they reach further stages of the pipeline.\\

The steps are:
\begin{itemize}
	\item[1] \emph{Occludee Generation}: Select occluders and generate simplified volumes.
	\item[2] \emph{Occlusion Map Generation}: Render occluder simplified volumes into the Occlusion Map Texture.
	\item[3] \emph{Visibility Testing}: Determine which occludees are occluded and stored them in the Visibility Map.
	\item[4] \emph{Vertex Discard}: Cull all the vertices that belong to invisible occludees.
\end{itemize}


%------------------------------------------------------------------------- 
\subsection{Occlusion Map Generation}
%
The method begins Offline by creating a database of selected occluders that meet a predefined criteria \cite{cacic_occlusion_1}, 
and storing the proxy meshes which are simplified, low-poly and conservative versions of the original occluders. 
These simplified occluders will be rendered faster than the original meshes at the expense of more conservativeness.\\

In each frame, object-precision culling techniques such as Frustum Culling, PVS and Portal Culling \cite{survey_visibility} are applied 
to discard as much occluders as possible. 
With this obtained reduced subset of occluders we perform the first step of the method which is to render the proxy meshes into the \emph{Occlusion Map}. 
This buffer stores the closest to camera depth values of every rasterized occluder and is implemented as a 32 bit floating point render target texture 
which is preferably a one fourth downscaled version of the screen framebuffer. \\

Unlike the HOM�s Occlusion Map \cite{hom}, our map does not contain opacity information, therefore the buffer is more similar 
to the Hierarchical Z-Buffer \cite{Hierarchical_zbuffer} which only stores the depth values of the occluders in 
each point, leaving the highest depth value to indicate no occluder presence.\\

The generation of the Occlusion Map is relatively inexpensive as the GPU massively parallel power is utilized to render the low-poly convex volumes of 
the proxy meshes and also because the pixel shader applied is extremely straightforward because it only outputs the depth value of each point.

\subimages[!hbp]
{First step is to obtain the simplified occluders as proxy meshes. Second step is to render all proxy meshes to the \emph{Occlusion Map} texture.}
{occluder_generation}{
\subimage[]{}{occluder_generation.png}%
}


%------------------------------------------------------------------------- 
\subsection{Visibility Test}
%
The core of this image based Occlusion Culling algorithm is to perform the Visibility Test for each selected occludee against the fusion of all the occluders represented by the Occlusion Map. 
Then it is used to determine whether the occludee geometry will continue along the pipeline or if it will be culled immediately.
Visibility testing is performed by contrasting the points inside the occludee screen space bounding rectangle against the \emph{Occlusion Map} depth values that contain the aggregated information of the occluders.  
In each frame, for every occludee in the viewing frustum, the algorithm performs a screen space projection of the occludee bounding box vertices. 
With those eight screen projected points, it determines the clipped 2D screen space bounding rectangle and finds the furthest from camera depth value of those extreme points. 
The resulting occludee bounding rectangle is a conservative superset of the actual pixels covered by the occludee. 
Then actual visibility test tries to determine if the occludee would actually contribute to the final image and starts by comparing all 
the depth values inside the occludee bounding rectangle against the ones in the \emph{Occlusion Map}; when at least one point of the occludee 
is closer to the camera than the one stored in the position in the \emph{Occlusion Map}, the algorithm can assume that such point is visible, 
and that the occludee as a whole can be considered potentially visible. 
On the other hand, to determine that an occludee is completely culled, all the pixels must be examined exhaustively and proved to be farther than the values stored in the \emph{Occlusion Map}.\\
 
Some methods perform this Overlap and Depth Test in CPU \cite{frostbite} [CITAR Intel] \cite{lazy_occlusion_grid} \cite{cacic_occlusion_2},  
and others use special GPU hardware capabilities such as hardware occlusion queries [CITAR Nvidia], \cite{occlusion_queries} or the more modern 
predicate/conditional rendering [CITAR]. 
Our method manually computes the visibility result pixel by pixel utilizing HLSL pixel shaders.\\

However as explained before, to actually conclude that a occludee is culled, we have to exhaustively test all the pixels inside the occludee bounding 
rectangle, resulting in $N \times M$ texture fetches to the \emph{Occlusion Map}. 
As the screen space regions covered by the occludees get larger, the number of texels to fetch and test can reach very high numbers.\\

To accelerate this, some methods build a pyramid of downsized versions of the \emph{Occlusion Map} where each increasing level is half the size of the previous one. 
There are two approaches to utilize the pyramid, one is like the method used in HOM \cite{hom} and HZB \cite{Hierarchical_zbuffer} which they begin at 
some level of the pyramid depending on the occludee bounding rectangle size and have to go to the finest level to assure that the occludee is completely 
culled by the occluders.
The other approach [CITAR Darnell] [CITAR R�kos] only sticks to a selected level of the pyramid, limiting the possible number of texture samples to a given 
constant to avoid the worst case scenario where they have to move to levels with greater detail. 
After implementing this last variation we found that the level of conservativeness was higher than expected for medium to large screen space occludees.\\

In this work we found that using a single level \emph{Occlusion Map} of a fourth of the original screen buffer was a good tradeoff between number of texture samples 
and level of conservativeness. In the next section we discuss the methods used to leverage the GPU hardware to perform this visibility test.


\subimages[!hbp]
{The occludees in the scene are projected in 2D and the Bounding Rectangle is calculated. 
	For each rectangle the algorithm performs the visibility test in GPU accessing the \emph{Occlusion Map}, 
	storing visibility result in the visibility map.}
{visibility_test}{
\subimage[]{}{visibility_test.png}%
}



%------------------------------------------------------------------------- 
\subsection{Block subdivision}
%
Despite having a downsized version of the \emph{Occlusion Map}, performing all the $N \times M$ texture samples in a single Pixel Shader execution 
does not perform as expected, because of the serial nature of the algorithm presented in figure \ref{img_visibility_test}. 
In the best cases this inner loop could take only a few cycles whereas in other cases the same execution could take hundreds of thousands of cycles before it is finished.\\

For this reason, in our method the visibility test is parallelized, taking advantage of the parallel execution of the pixel shaders, 
splitting the total region covered by each occludee bounding rectangle into a series of fixed size blocks, where each one only performs 
a maximum of $8 \times 8$ texture lookups to the \emph{Occlusion Map}. 
This way each occludee bounding rectangle is split up in  blocks that concurrently perform the visibility test by executing pixel shaders that 
output a $0$ color value meaning the block itself is completely occluded or $1$ if the block is potentially visible. 
The output of each visibility test block goes to a rendering target texture called \emph{Unreduced Visibility Map (UVM)} that holds 
the block visibility results one next to the other as seen in figure \ref{img_visibility_test}. 
Thus every occludee tested in the scene has an reserved region inside this map to hold the results of each of its tested blocks.\\

In order to simplify the way each region is assigned, every occludee is assumed to have a fixed number of blocks, no matter its screen space size. 
In our study we determined that every occludee would have a preset number of $32 \times 32$ blocks assigned, resulting in a total of $1024$ blocks. 
This gives us a maximum occludee screen size of $256 \times 256$ pixels and if the dimensions are larger than those, the occludee is simply considered potentially visible.\\

To implement this algorithm using shader model 3 (without compute shaders), we carefully position a $32 \times 32$ pixel quad (GPGPU quad) and render it using a pixel 
shader that executes the visibility test code. 
Each pixel of this quad represents a block visibility test of the occluder. 
The shader gets the occludee bounding rectangle coordinates, depth value and the block number as parameters, and then executes the $8 \times 8$ pixels 
overlap and depth test.\\

\begin{figure}[!hbp]
	\begin{algorithmic}[1]
		\REQUIRE $occludeeSize$
		\REQUIRE $occludeePos$ \COMMENT{occludee AABB position}
		\REQUIRE $depthMap$
		\REQUIRE $pos$ \COMMENT{quad texture coordinates}
		\REQUIRE $quadSize$
		\STATE $base \leftarrow occludeePos \times pos + quadSize \times 8$
		\STATE $result \leftarrow 0$ \COMMENT{not visible}
		\FOR{$i=0$ to $8$}
			\FOR{$j=0$ to $8$}
				\STATE $p \leftarrow base + (i, j)$
				\STATE $depth \leftarrow$ read p from depthMap
				\IF{$occludeeDepth \geq depth$}
					\STATE $result \leftarrow 1$ \COMMENT{visible}
					\STATE break
				\ENDIF
			\ENDFOR
		\ENDFOR
	\end{algorithmic}
	\caption{Visibility test algorithm performed in a Pixel Shader}
\end{figure}

Using this block subdivision strategy, the visibility test is split into smaller task units and performed in parallel making use of the available GPU shader execution cores. 
If all the blocks comprising the occludee rectangle output $0$ values, then the whole occludee is considered culled, conversely when at 
least one of the blocks results visible the whole occludee is considered potentially visible.\\

\begin{figure}[!hbp]
	\oneimage{The occludee is split into 8x8 blocks, then each block performs the visibility test and stores the result into the \emph{Unreduced Visibility Map}. Each occludee has an pre-assigned region of 32x32 blocks inside this Map.  \emph{Occlusion Map} texture.}{.99}{visibility_blocks.png}
\end{figure}


Nevertheless the visibility result of each occludee is not consolidated into a single value, but spread into a series of $32 \times 32$ matrices inside some region of the \emph{UVM}. 
The next step of our method reduces each $32 \times 32$ occludee visibility result matrix into a consolidated \emph{Visibility Map} that will hold the results of each visibility test one next to the other.\\

%------------------------------------------------------------------------- 
\subsection{Visibility Map Reduction}
%
In order to reduce the \emph{UVM} and consolidate each $32 \times 32$ region into a single value, we need to determine if there is at least a non-zero value inside that matrix. 
To achieve this, we search for the maximum value of the matrix to see if there is any value other than zero. 
The search is done utilizing a parallel reduction approach with two rendering passes to limit the total number of operations. 
In the first pass we search the maximum value in each matrix column of $32$ pixels and store it in an intermediate texture. 
In the second pass, we obtain the final \emph{Visibility Map} looking for the maximum value in each row. 
Finally we end up with the \emph{Visibility Map} containing the results of the occlusion culling process for each occludee tested in the current frame, which will be heavily accessed in the next step of our method. 


%------------------------------------------------------------------------- 
\subsection{Vertex Discard}
%
This \emph{Visibility Map} texture could be sent back to the CPU and processed there to avoid having to execute the draw calls to occluded objects; 
however this would produce a stalling effect on the GPU while sending the results back. 
To address this issue, we propose an asynchronous mechanism where the CPU does not need the results of the visibility test.\\

In our method the CPU always performs the draw calls for all the geometry that is potentially visible 
(the subset that passes frustum culling, portal culling, PVS, etc), and the GPU is responsible for discarding the occluded geometry based 
on the \emph{Visibility Map} content.\\

In our implementation we slightly modify the vertex shader that performs the \emph{World-View-Projection} transformation. 
Before drawing an occludee, we send a parameter to the pixel shader indicating the \emph{ID} of occludee that is about to be rendered. 
Based on that value, the vertex shader will perform a texture lookup in the \emph{Visibility Map} to find the occlusion status for that particular occludee. 
If it is potentially visible, then the vertex shader does its usual computation letting the vertex continue throughout the pipeline. 
On the other hand, if the occludee is invisible we assign a negative Z value to the output vertex to let the GPU know that it has to be culled. 
This process is performed for every vertex that constitutes the occludee geometry.

\begin{figure}[!hbp]
	\begin{algorithmic}[1]
		\REQUIRE $vp$ \COMMENT{Vertex 3D Position}
		\REQUIRE $vMap$ \COMMENT{Visibility Map}
		\REQUIRE $i$ \COMMENT{Occludee index}
		\STATE $vis \leftarrow$ read visibility info from $vMap$ using $i$
		\IF{$vis$ = 0}
			\STATE \COMMENT{Continue with normal vertex shader calculations}
		\ELSE
			\STATE $vp.z = -1$ \COMMENT{Discard vertex}
		\ENDIF
	\end{algorithmic}
	\caption{Visibility discard algorithm performed in a Vertex Shader}
\end{figure}





%==========================================
\section{Implementation and Results}
%
Our method was implemented using C\# 4.0 with DirectX 9 and Shader Model 3. 
We decided not to use newer shader models (with Compute Shader capabilities) so we could test in the current commodity hardware. 
The implementation of our occlusion culling module was designed in such way to be easily adapted to other graphics frameworks, where only certain parts have to be added or modified.\\

We tested our method in a densely occluded 3D city scene, composed of 210 meshes, adding up a total of 379.664 triangles. 
For this scene 258 occluder proxies were generated in Offline time based on the ideas presented by \cite{cacic_occlusion_1}. 

\begin{figure}[!hbp]
	\oneimage{Top: The 3D city scene used to test the algorithm. Bottom: The simplified occluder set used for Occlusion Culling}
	{.99}{city_images.png}
\end{figure}

In order to analyze the algorithm performance, 15 representative scene View Points were taken, where in each position we compute the following 
occlusion metric:
\[
	Value = (\frac{t - v}{t}) \times 100
\]
Where $t$ is the total scene meshes and $v$ is the total visible meshes. \\
With this metric we can determine the percentage of discarded meshes that were not sent to the GPU in each frame due to occlusion culling.\\

These values are computed with Occlusion Culling deactivated and then with it activated. 
We also include the frames per second that resulted from rendering the scene with and without Occlusion Culling. 
The results were computed using a PC with Intel Core i3 2.40GHz processor with 2GB RAM and Intel HD Graphics 3000 GPU.

\begin{figure}[!hbp]
	\oneimage{FPS rendering performance only with Frustum Culling and then with Occlusion Culling activated, at the fifteen different selected View Points.}
	{.99}{discarded_objects_percent.png}
\end{figure}

\begin{figure}[!hbp]
	\oneimage{Discarded mesh percent, first with only Frustum Culling and then activating Occlusion Culling, at the fifteen different selected View Points.}
	{.99}{discarded_objects_fps.png}
\end{figure}




%==========================================
\section{Conclusions and Future Work}
%
We have implemented a method that performs image space occlusion culling completely in GPU, taking advantage of its rendering power to 
build the \emph{Occlusion Map} and leveraging its parallel architecture to perform the visibility test.\\

According to our results, this occlusion culling method is applicable in densely occluded scenes where pixel shaders are computationally 
expensive and specifically if they alter the default depth value of the fragments, like in \cite{z_correct_bump_1} and \cite{z_correct_bump_2}. 
Conversely we found that for scenes with lightweight pixel shaders and no depth overrides, our method performs similar to the GPU built-in Early Z 
culling, making it suitable for mixed case scenarios.\\

As our implementation is based on Shader Model 3, it does not require special hardware requirements, beyond the vertex shader texture lookup 
capabilities present in most commodity hardware GPUs. 
For some older hardware, particularly those without Unified Shader architecture, the vertex texture lookup may downgrade the performance 
significantly [CITAR].\\

It is also important to have some considerations before applying this technique. 
As all the occludees are sent to the GPU, no matter if they are occluded or not, there is a CPU-GPU bus bandwidth required to transfer the primitives to the graphic adapter.
Moreover, as many other similar occlusion culling algorithms, the occluders have to be preprocessed in order to simplify the geometry into simpler conservative volumes.\\
 
Among the numerous enhancements to be made to our method, we would like to modify it to overcome the limitaton of the 
$256 \times 256$ pixel size occludees and to explore built in hardware options to reduce the \emph{UVM}, avoiding the current two rendering pass method.\\

Finally as newer versions of DirectX and OpenGL become available we could explore the option of implementing this method using compute shaders, 
orienting it  to the work presented by Nie�ner et al \cite{occlusion_culling_tessellation} and [CITAR R�kos]. 
We could also count the number of visible blocks in each occludee and utilize the results to determine some level of detail in Geometry and Pixel Shaders.



%==========================================
\iffinal
% use section* for acknowledgement
\section*{Acknowledgment}
%
The authors would like to thank the GIGC Computer Graphics Research Group for supporting this research, and the Department of Information Systems 
Engineering for providing the support and funding. We also thank Retrovia Project Marta Garcen, Eva Ferrari (English node) for reviewing our work 
and to the Algebra and Analytic Geometry node to make this contact possible.
\fi



%==========================================

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

\bibliographystyle{IEEEtran}
\bibliography{bib_extra/IEEEabrv,example}

%\begin{thebibliography}{1}

%\bibitem{sibgrapi2013}
%\emph{Sibgrapi 2013, Proceedings of the XXVI Brazilian Symposium on Computer Graphics and Image Processing}.\hskip 1em plus 0.5em minus 0.4em\relax  Arequipa, Per{\'u}: {IEEE}, august 2013.

%\end{thebibliography}




\begin{figure}[!hbp]
	\oneimage{Top: The original city scene without Occlusion Culling. Middle: The scene with Occlusion Culling. Bottom: The yellow box shows the occluded meshes of the scene.}
	{.99}{final_images.png}
\end{figure}





\end{document}

%==========================================
